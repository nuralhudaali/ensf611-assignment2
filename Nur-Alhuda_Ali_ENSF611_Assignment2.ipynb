{
 "cells": [
  {
   "attachments": {},
   "cell_type": "markdown",
   "id": "92778525",
   "metadata": {},
   "source": [
    "# Assignment 2: Linear Models and Validation Metrics (30 marks total)\n",
    "### Due: October 10 at 11:59pm\n",
    "\n",
    "### Name: Nur-Alhuda Ali"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "id": "ce31b39a",
   "metadata": {},
   "source": [
    "### In this assignment, you will need to write code that uses linear models to perform classification and regression tasks. You will also be asked to describe the process by which you came up with the code. More details can be found below. Please cite any websites or AI tools that you used to help you with this assignment."
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "id": "f7c6de86",
   "metadata": {},
   "source": [
    "## Part 1: Classification (14.5 marks total)\n",
    "\n",
    "You have been asked to develop code that can help the user determine if the email they have received is spam or not. Following the machine learning workflow described in class, write the relevant code in each of the steps below:"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "id": "7e3c6fc8",
   "metadata": {},
   "source": [
    "### Step 0: Import Libraries"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 55,
   "id": "33f86925",
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import pandas as pd"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "id": "5f9d33a8",
   "metadata": {},
   "source": [
    "### Step 1: Data Input (1 mark)\n",
    "\n",
    "The data used for this task can be downloaded using the yellowbrick library: \n",
    "https://www.scikit-yb.org/en/latest/api/datasets/spam.html\n",
    "\n",
    "Use the yellowbrick function `load_spam()` to load the spam dataset into the feature matrix `X` and target vector `y`.\n",
    "\n",
    "Print the size and type of `X` and `y`."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 56,
   "id": "33583c67",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Size of X:  262200 elements\n",
      "Shape of X:  (4600, 57)\n",
      "Type of X:\n",
      " word_freq_make                float64\n",
      "word_freq_address             float64\n",
      "word_freq_all                 float64\n",
      "word_freq_3d                  float64\n",
      "word_freq_our                 float64\n",
      "word_freq_over                float64\n",
      "word_freq_remove              float64\n",
      "word_freq_internet            float64\n",
      "word_freq_order               float64\n",
      "word_freq_mail                float64\n",
      "word_freq_receive             float64\n",
      "word_freq_will                float64\n",
      "word_freq_people              float64\n",
      "word_freq_report              float64\n",
      "word_freq_addresses           float64\n",
      "word_freq_free                float64\n",
      "word_freq_business            float64\n",
      "word_freq_email               float64\n",
      "word_freq_you                 float64\n",
      "word_freq_credit              float64\n",
      "word_freq_your                float64\n",
      "word_freq_font                float64\n",
      "word_freq_000                 float64\n",
      "word_freq_money               float64\n",
      "word_freq_hp                  float64\n",
      "word_freq_hpl                 float64\n",
      "word_freq_george              float64\n",
      "word_freq_650                 float64\n",
      "word_freq_lab                 float64\n",
      "word_freq_labs                float64\n",
      "word_freq_telnet              float64\n",
      "word_freq_857                 float64\n",
      "word_freq_data                float64\n",
      "word_freq_415                 float64\n",
      "word_freq_85                  float64\n",
      "word_freq_technology          float64\n",
      "word_freq_1999                float64\n",
      "word_freq_parts               float64\n",
      "word_freq_pm                  float64\n",
      "word_freq_direct              float64\n",
      "word_freq_cs                  float64\n",
      "word_freq_meeting             float64\n",
      "word_freq_original            float64\n",
      "word_freq_project             float64\n",
      "word_freq_re                  float64\n",
      "word_freq_edu                 float64\n",
      "word_freq_table               float64\n",
      "word_freq_conference          float64\n",
      "char_freq_;                   float64\n",
      "char_freq_(                   float64\n",
      "char_freq_[                   float64\n",
      "char_freq_!                   float64\n",
      "char_freq_$                   float64\n",
      "char_freq_#                   float64\n",
      "capital_run_length_average    float64\n",
      "capital_run_length_longest      int64\n",
      "capital_run_length_total        int64\n",
      "dtype: object \n",
      "\n",
      "Size of y:  4600\n",
      "Type of y:  int64 \n",
      "\n"
     ]
    }
   ],
   "source": [
    "# TO DO: Import spam dataset from yellowbrick library\n",
    "# TO DO: Print size and type of X and y\n",
    "\n",
    "from yellowbrick.datasets import load_spam\n",
    "X, y = load_spam() # returns features into X and target into y\n",
    "\n",
    "print(\"Size of X: \", X.size, \"elements\")\n",
    "print(\"Shape of X: \", X.shape)\n",
    "print(\"Type of X:\\n\", X.dtypes, \"\\n\")\n",
    "\n",
    "print(\"Size of y: \", y.size)\n",
    "print(\"Type of y: \", y.dtypes, \"\\n\")"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "id": "156db208",
   "metadata": {},
   "source": [
    "### Step 2: Data Processing (1.5 marks)\n",
    "\n",
    "Check to see if there are any missing values in the dataset. If necessary, select an appropriate method to fill-in the missing values."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 42,
   "id": "4e7204f5",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Number of nulls in feature matrix:\n",
      " word_freq_make                0\n",
      "word_freq_labs                0\n",
      "word_freq_857                 0\n",
      "word_freq_data                0\n",
      "word_freq_415                 0\n",
      "word_freq_85                  0\n",
      "word_freq_technology          0\n",
      "word_freq_1999                0\n",
      "word_freq_parts               0\n",
      "word_freq_pm                  0\n",
      "word_freq_direct              0\n",
      "word_freq_cs                  0\n",
      "word_freq_meeting             0\n",
      "word_freq_original            0\n",
      "word_freq_project             0\n",
      "word_freq_re                  0\n",
      "word_freq_edu                 0\n",
      "word_freq_table               0\n",
      "word_freq_conference          0\n",
      "char_freq_;                   0\n",
      "char_freq_(                   0\n",
      "char_freq_[                   0\n",
      "char_freq_!                   0\n",
      "char_freq_$                   0\n",
      "char_freq_#                   0\n",
      "capital_run_length_average    0\n",
      "capital_run_length_longest    0\n",
      "word_freq_telnet              0\n",
      "word_freq_lab                 0\n",
      "word_freq_address             0\n",
      "word_freq_650                 0\n",
      "word_freq_all                 0\n",
      "word_freq_3d                  0\n",
      "word_freq_our                 0\n",
      "word_freq_over                0\n",
      "word_freq_remove              0\n",
      "word_freq_internet            0\n",
      "word_freq_order               0\n",
      "word_freq_mail                0\n",
      "word_freq_receive             0\n",
      "word_freq_will                0\n",
      "word_freq_people              0\n",
      "word_freq_report              0\n",
      "word_freq_addresses           0\n",
      "word_freq_free                0\n",
      "word_freq_business            0\n",
      "word_freq_email               0\n",
      "word_freq_you                 0\n",
      "word_freq_credit              0\n",
      "word_freq_your                0\n",
      "word_freq_font                0\n",
      "word_freq_000                 0\n",
      "word_freq_money               0\n",
      "word_freq_hp                  0\n",
      "word_freq_hpl                 0\n",
      "word_freq_george              0\n",
      "capital_run_length_total      0\n",
      "dtype: int64 \n",
      "\n",
      "Number of nulls in target vector:  0\n"
     ]
    }
   ],
   "source": [
    "# TO DO: Check if there are any missing values and fill them in if necessary\n",
    "print(\"Number of nulls in feature matrix:\\n\", X.isnull().sum().sort_values(ascending=False), \"\\n\")\n",
    "print(\"Number of nulls in target vector: \", y.isnull().sum())"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "id": "a489285a",
   "metadata": {},
   "source": [
    "For this task, we want to test if the linear model would still work if we used less data. Use the `train_test_split` function from sklearn to create a new feature matrix named `X_small` and a new target vector named `y_small` that contain **5%** of the data."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 43,
   "id": "f9bc4a23",
   "metadata": {},
   "outputs": [],
   "source": [
    "# TO DO: Create X_small and y_small \n",
    "from sklearn.model_selection import train_test_split\n",
    "\n",
    "X_big, X_small, y_big, y_small = train_test_split(X, y, test_size=0.05, random_state=5)"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "id": "70e6c46f",
   "metadata": {},
   "source": [
    "### Step 3: Implement Machine Learning Model\n",
    "\n",
    "1. Import `LogisticRegression` from sklearn\n",
    "2. Instantiate model `LogisticRegression(max_iter=2000)`.\n",
    "3. Implement the machine learning model with three different datasets: \n",
    "    - `X` and `y`\n",
    "    - Only first two columns of `X` and `y`\n",
    "    - `X_small` and `y_small`"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "id": "b89f3d84",
   "metadata": {},
   "source": [
    "### Step 4: Validate Model\n",
    "\n",
    "Calculate the training and validation accuracy for the three different tests implemented in Step 3"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "id": "352106a3",
   "metadata": {},
   "source": [
    "### Step 5: Visualize Results (4 marks)\n",
    "\n",
    "1. Create a pandas DataFrame `results` with columns: Data size, training accuracy, validation accuracy\n",
    "2. Add the data size, training and validation accuracy for each dataset to the `results` DataFrame\n",
    "3. Print `results`"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 57,
   "id": "be4b5c0a",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "FOR DATASET: X and y\n",
      "Training score: 0.93\n",
      "Validation score: 0.94\n",
      "\n",
      "FOR DATASET: First two columns of X and y\n",
      "Training score: 0.61\n",
      "Validation score: 0.61\n",
      "\n",
      "FOR DATASET: X_small and y_small\n",
      "Training score: 0.97\n",
      "Validation score: 0.81\n",
      "\n",
      "RESULTS TABLE\n",
      "-----------------------------------------------------------------------\n",
      "                      Data Size  Training Accuracy  Validation Accuracy\n",
      "X & y                    262200           0.928696             0.937391\n",
      "First 2 col of X & y       9200           0.608406             0.613043\n",
      "X_small & y_small         13110           0.970930             0.810345\n"
     ]
    }
   ],
   "source": [
    "# TO DO: ADD YOUR CODE HERE FOR STEPS 3-5\n",
    "# Note: for any random state parameters, you can use random_state = 0\n",
    "# HINT: USING A LOOP TO STORE THE DATA IN YOUR RESULTS DATAFRAME WILL BE MORE EFFICIENT\n",
    "\n",
    "from sklearn.linear_model import LogisticRegression\n",
    "model_logreg = LogisticRegression(max_iter=2000)\n",
    "\n",
    "# Dataset: X and y\n",
    "X_train, X_test, y_train, y_test = train_test_split(X, y, random_state=0)\n",
    "model1 = model_logreg.fit(X_train, y_train)\n",
    "\n",
    "size1 = X.size\n",
    "training_score1 = model1.score(X_train, y_train)\n",
    "validation_score1 = model1.score(X_test, y_test)\n",
    "datasets = {'X & y' : [size1, training_score1, validation_score1]}  # add info to dictonary (used to create results table)\n",
    "\n",
    "print(\"FOR DATASET: X and y\")\n",
    "print(\"Training score: {:.2f}\".format(training_score1))\n",
    "print(\"Validation score: {:.2f}\".format(validation_score1))\n",
    "\n",
    "# Dataset: First 2 Columns of X, and y\n",
    "X_twocol = X.iloc[:, :2]\n",
    "X_train, X_test, y_train, y_test = train_test_split(X_twocol, y, random_state=0)\n",
    "model2 = model_logreg.fit(X_train, y_train)\n",
    "\n",
    "size2 = X_twocol.size\n",
    "training_score2 = model2.score(X_train, y_train)\n",
    "validation_score2 = model2.score(X_test, y_test)\n",
    "datasets['First 2 col of X & y'] = [size2, training_score2, validation_score2]  # add info to dictionary\n",
    "\n",
    "print(\"\\nFOR DATASET: First two columns of X and y\")\n",
    "print(\"Training score: {:.2f}\".format(training_score2))\n",
    "print(\"Validation score: {:.2f}\".format(validation_score2))\n",
    "\n",
    "# Dataset: X_small and y_small\n",
    "X_train, X_test, y_train, y_test = train_test_split(X_small, y_small, random_state=0)\n",
    "model3 = model_logreg.fit(X_train, y_train)\n",
    "\n",
    "size3 = X_small.size\n",
    "training_score3 = model3.score(X_train, y_train)\n",
    "validation_score3 = model3.score(X_test, y_test)\n",
    "datasets['X_small & y_small'] = [size3, training_score3, validation_score3] # add info to dictionary\n",
    "\n",
    "print(\"\\nFOR DATASET: X_small and y_small\")\n",
    "print(\"Training score: {:.2f}\".format(training_score3))\n",
    "print(\"Validation score: {:.2f}\".format(validation_score3))\n",
    "\n",
    "# Visualization\n",
    "results = pd.DataFrame.from_dict(datasets, orient='index', columns=['Data Size', 'Training Accuracy', 'Validation Accuracy'])\n",
    "print(\"\\nRESULTS TABLE\")\n",
    "print(\"-----------------------------------------------------------------------\")\n",
    "print(results)"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "id": "d4427d4f",
   "metadata": {},
   "source": [
    "### Questions (4 marks)\n",
    "1. How do the training and validation accuracy change depending on the amount of data used? Explain with values.\n",
    "2. In this case, what do a false positive and a false negative represent? Which one is worse?\n",
    "\n",
    "*YOUR ANSWERS HERE*\n",
    "1. Using only the first 2 columns (features) of X, with a data size of 9200 elements, resulted in almost equal training and validation accuracies that were low, which means the model had high bias and low variance. This is because the model only used 2 features from the 57 available features, so it did not account for the complete behaviour of the data.\n",
    "\n",
    "    Using 5% of the data, with a data size of 13,110 elements, increased the training and validation accuracies significantly. This is because all the 57 features were used in the model, despite the data used being only 5% of the total data available, allowing the model to account for the complete behaviour of the data (weight of the features).\n",
    "\n",
    "    Using the entire dataset, with a data size of 262,200 elements, the validation accuracy is the highest of all 3 datasets. The training accuracy is slightly lower than the validation accuracy, likely due to the train-test split. Increasing the size of the test set would fix this issue.\n",
    "\n",
    "    Based on these observations, we can conclude that increasing the dataset size and the number of features used in the model improves the performance of the model as it better informs the model about the behaviour (both local and general) of the data.\n",
    "\n",
    "2. A false positive represents an email that is flagged as spam when it actually is not. A false negative is an email that is NOT flagged as spam, when it actually is.\n",
    "\n",
    "    In this case, a false positive is worse because if good mail is flagged as spam, then the email user could miss important/time-sensitive emails that go to the spam folder."
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "id": "7559517a",
   "metadata": {},
   "source": [
    "### Process Description (4 marks)\n",
    "Please describe the process you used to create your code. Cite any websites or generative AI tools used. You can use the following questions as guidance:\n",
    "1. Where did you source your code?\n",
    "1. In what order did you complete the steps?\n",
    "1. If you used generative AI, what prompts did you use? Did you need to modify the code at all? Why or why not?\n",
    "1. Did you have any challenges? If yes, what were they? If not, what helped you to be successful?"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "id": "59fe687f",
   "metadata": {},
   "source": [
    "*DESCRIBE YOUR PROCESS HERE*\\\n",
    "\\\n",
    "I used the ENSF 611 Jupyter notebooks as a general guideline for how to use SciKit-Learn, but other than that, I created my own code. I completed the steps in order and before every step, I would have a general idea of what libraries and functions I would need to use, so I would study the documentation for SciKit-Learn (https://scikit-learn.org/stable/modules/generated/sklearn.linear_model.LogisticRegression.html) to understand what the functions do, what they return, and what their arguments are not only to ensure that I am using the correct syntax, but also getting the results I want.\n",
    "\n",
    "I also referred to the ENSF 611 Model Selection Jupyter and Linear Classification notebooks to help me with understanding and interpretting my results. I generally don't like to use generative AI to help me code as I prefer to do more manual research and read the documentation for libraries that I am using myself - so I did not use any generative AI for this process.\n",
    "\n",
    "One thing I found challenging was creating the results dataframe in Step 5 efficiently since it had been a few months since I have used Pandas. I already had the idea of using a dictionary to store the results with the dataset description as the key, but I had forgotton how to turn that into a database. Initially, I tried to use nested for-loops, which led me to think that there must be an easier way to do it. So I read the Pandas documentation and found a quick and easy solution using the function pd.DataFrame.from_dict() ."
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "id": "fb4c78a8",
   "metadata": {},
   "source": [
    "## Part 2: Regression (10.5 marks total)\n",
    "\n",
    "For this section, we will be evaluating concrete compressive strength of different concrete samples, based on age and ingredients. You will need to repeat the steps 1-4 from Part 1 for this analysis."
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "id": "b2ba83c5",
   "metadata": {},
   "source": [
    "### Step 1: Data Input (1 mark)\n",
    "\n",
    "The data used for this task can be downloaded using the yellowbrick library: \n",
    "https://www.scikit-yb.org/en/latest/api/datasets/concrete.html\n",
    "\n",
    "Use the yellowbrick function `load_concrete()` to load the spam dataset into the feature matrix `X` and target vector `y`.\n",
    "\n",
    "Print the size and type of `X` and `y`."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 58,
   "id": "6ff2e34f",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Size of X:  8240 elements\n",
      "Shape of X:  (1030, 8)\n",
      "Type of X:\n",
      " cement    float64\n",
      "slag      float64\n",
      "ash       float64\n",
      "water     float64\n",
      "splast    float64\n",
      "coarse    float64\n",
      "fine      float64\n",
      "age         int64\n",
      "dtype: object \n",
      "\n",
      "Size of y:  1030\n",
      "Type of y:  float64 \n",
      "\n"
     ]
    }
   ],
   "source": [
    "# TO DO: Import spam dataset from yellowbrick library\n",
    "# TO DO: Print size and type of X and y\n",
    "from yellowbrick.datasets import load_concrete\n",
    "X, y = load_concrete()\n",
    "\n",
    "print(\"Size of X: \", X.size, \"elements\")\n",
    "print(\"Shape of X: \", X.shape)\n",
    "print(\"Type of X:\\n\", X.dtypes, \"\\n\")\n",
    "\n",
    "print(\"Size of y: \", y.size)\n",
    "print(\"Type of y: \", y.dtypes, \"\\n\")"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "id": "c5294cfa",
   "metadata": {},
   "source": [
    "### Step 2: Data Processing (0.5 marks)\n",
    "\n",
    "Check to see if there are any missing values in the dataset. If necessary, select an appropriate method to fill-in the missing values."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 59,
   "id": "693c5fa3",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Number of nulls in X:\n",
      " cement    0\n",
      "slag      0\n",
      "ash       0\n",
      "water     0\n",
      "splast    0\n",
      "coarse    0\n",
      "fine      0\n",
      "age       0\n",
      "dtype: int64 \n",
      "\n",
      "Number of nulls in y:  0\n"
     ]
    }
   ],
   "source": [
    "# TO DO: Check if there are any missing values and fill them in if necessary\n",
    "print(\"Number of nulls in X:\\n\", X.isnull().sum().sort_values(ascending=False), \"\\n\")\n",
    "print(\"Number of nulls in y: \", y.isnull().sum())"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "id": "1bc60489",
   "metadata": {},
   "source": [
    "### Step 3: Implement Machine Learning Model (1 mark)\n",
    "\n",
    "1. Import `LinearRegression` from sklearn\n",
    "2. Instantiate model `LogisticRegression(max_iter=2000)`.\n",
    "3. Implement the machine learning model with `X` and `y`"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 70,
   "id": "b5041945",
   "metadata": {},
   "outputs": [],
   "source": [
    "# TO DO: ADD YOUR CODE HERE\n",
    "# Note: for any random state parameters, you can use random_state = 0\n",
    "from sklearn.linear_model import LinearRegression\n",
    "\n",
    "X_train, X_test, y_train, y_test = train_test_split(X, y, random_state=0)\n",
    "lin_reg = LinearRegression().fit(X_train, y_train)\n"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "id": "1de28482",
   "metadata": {},
   "source": [
    "### Step 4: Validate Model (1 mark)\n",
    "\n",
    "Calculate the training and validation accuracy using mean squared error and R2 score."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 71,
   "id": "970c038b",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "USING THE R2 SCORE:\n",
      "Training score: 0.61\n",
      "Validation score: 0.62\n",
      "\n",
      "USING MEAN-SQUARED ERROR:\n",
      "Training score: 111.36\n",
      "Validation score: 95.90\n"
     ]
    }
   ],
   "source": [
    "# TO DO: ADD YOUR CODE HERE\n",
    "from sklearn.metrics import r2_score, mean_squared_error\n",
    "\n",
    "y_pred_train = lin_reg.predict(X_train)\n",
    "y_pred_test = lin_reg.predict(X_test)\n",
    "\n",
    "# Using R2 score:\n",
    "train_score_R2 = r2_score(y_train, y_pred_train)\n",
    "test_score_R2 = r2_score(y_test, y_pred_test)\n",
    "\n",
    "print(\"USING THE R2 SCORE:\")\n",
    "print(\"Training score: {:.2f}\".format(train_score_R2))\n",
    "print(\"Validation score: {:.2f}\".format(test_score_R2))\n",
    "\n",
    "scores = {'R2 Score' : [train_score_R2, test_score_R2]} # add results to dictionary (for visualization in next step)\n",
    "\n",
    "# Using Mean Squared Error:\n",
    "train_score_MSE = mean_squared_error(y_train, y_pred_train)\n",
    "test_score_MSE = mean_squared_error(y_test, y_pred_test)\n",
    "\n",
    "print(\"\\nUSING MEAN-SQUARED ERROR:\")\n",
    "print(\"Training score: {:.2f}\".format(train_score_MSE))\n",
    "print(\"Validation score: {:.2f}\".format(test_score_MSE))\n",
    "\n",
    "scores['MSE'] = [train_score_MSE, test_score_MSE] # add results to dictionary (for visualization in next step)"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "id": "54aa7795",
   "metadata": {},
   "source": [
    "### Step 5: Visualize Results (1 mark)\n",
    "1. Create a pandas DataFrame `results` with columns: Training accuracy and Validation accuracy, and index: MSE and R2 score\n",
    "2. Add the accuracy results to the `results` DataFrame\n",
    "3. Print `results`"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 72,
   "id": "88d223f3",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "          Training Accuracy  Validation Accuracy\n",
      "R2 Score           0.610823             0.623414\n",
      "MSE              111.358439            95.904136\n"
     ]
    }
   ],
   "source": [
    "# TO DO: ADD YOUR CODE HERE\n",
    "results = pd.DataFrame.from_dict(scores, orient='index', columns=['Training Accuracy', 'Validation Accuracy'])\n",
    "print(results)"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "id": "70a42bda",
   "metadata": {},
   "source": [
    "### Questions (2 marks)\n",
    "1. Did using a linear model produce good results for this dataset? Why or why not?\n",
    "\n",
    "    No, using a linear model did not produce good results. For the R2 score metric, values close to 1 indicate a better fitting model, whereas for the mean squared error metric, that is the case for values close to 0 (the opposite).\n",
    "\n",
    "    Using the R2 score metric, the training and validation accuracies were far from optimal and were also similar in value, 0.61 and 0.62, respectively, which means that the model is high-bias and does not account for the variance in (behaviour of) the data very well. Using the mean squared error metric, the training and validation accuracies were high, 111.36 and 95.90, respectively, indicating that the model does not make high quality predictions (has high prediction error).\n",
    "\n",
    "    One reason could be that the model is too regularized and the features need to be more represented in it. Another potential reason could be that the data has a lot of noise that has nothing to do with the features. Finally, it is possible that the data is simply better represented using a non-linear model."
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "id": "2ca0ff2f",
   "metadata": {},
   "source": [
    "### Process Description (4 marks)\n",
    "Please describe the process you used to create your code. Cite any websites or generative AI tools used. You can use the following questions as guidance:\n",
    "1. Where did you source your code?\n",
    "1. In what order did you complete the steps?\n",
    "1. If you used generative AI, what prompts did you use? Did you need to modify the code at all? Why or why not?\n",
    "1. Did you have any challenges? If yes, what were they? If not, what helped you to be successful?"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "id": "dfdb0880",
   "metadata": {},
   "source": [
    "*DESCRIBE YOUR PROCESS HERE*\\\n",
    "\\\n",
    "Like in *Part 1: Classification*, I used the ENSF 611 Jupyter notebooks as a guideline for how to use SciKit-Learn for model selection. Other than that, I relied on the SciKit-Learn documentation (https://scikit-learn.org/stable/modules/generated/sklearn.linear_model.LinearRegression.html#sklearn.linear_model.LinearRegression.predict) to aid me in writing my code, allowing me to understand that functions that I wanted to use so that I could properly implement them. I completed all the steps in order because every step relies on the one before it. However, after completing all the steps, I did go back to tweak certain parameters to see how that would affect my results in order to better understand what was going on.\n",
    "\n",
    "I used the same method for creating the results dataframe as in Part 1: Classificaton, using the Pandas documentation as reference. I did not use any generative AI for this assignment.\n",
    "\n",
    "A challenge I had was implementing the r2_score and mean_squared_error metrics to determine the training and validation accuracies since I was more familiar with using model.score() directly. However, I was able to figure out how to use these metrics by consulting the SciKit-Learn documentation (https://scikit-learn.org/stable/modules/generated/sklearn.metrics.mean_squared_error.html).\n"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "id": "e72ac3eb",
   "metadata": {},
   "source": [
    "## Part 3: Observations/Interpretation (3 marks)\n",
    "\n",
    "Describe any pattern you see in the results. Relate your findings to what we discussed during lectures. Include data to justify your findings.\n",
    "\n",
    "\n",
    "*ADD YOUR FINDINGS HERE*\n",
    "\n",
    "In *Part 1: Classification*, I found that increasing the data size and maximizing the number of features used to train the model makes for a better model. This is because increasing the data size reduces the effect of noise, which would cause overfitting, and allows the  model to learn the **general** behaviour and patterns of the data. This can be seen in the difference in results between using the full dataset X, which had training and validation scores of 0.93 and 0.94, respectively, versus only 5% of the data in X_small, which had a training score of 0.97 and a lower validation score of 0.81.\n",
    "\n",
    "Maximizing the number of features used to train the model allows the model to detect patterns and relationships among features. This can be seen in the difference in results between the full dataset X, which, again, had training and validation scores of 0.93 and 0.94, respectively, versus the dataset with only the first 2 features, resulting in training and validation scores of 0.61, indicating high bias and low variance.\n",
    "\n",
    "In *Part 2: Regression*, I found that if a model has lower training and validation accuracies using the R2-score metric, the same accuracies calculated using the mean squared error (MSE) metric will be high. In other words, the better the model is, the closer the accuracies calculated with the R2-score metric are to 1 (versus 0), and the lower the accuracies calculated with the mean squared error metric are. This can be seen in the results for this section's linear regression model, where the R2-score metric resulted in training and validation accuracies of 0.61 and 0.62, respectively, and the MSE metric resulted in training and validation accuracies of 111.36 and 95.90, respectively, both indicating that the linear regression model for this dataset is high-bias, and therefore requires a different model with less regularization."
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "id": "40b84eed",
   "metadata": {},
   "source": [
    "## Part 4: Reflection (2 marks)\n",
    "Include a sentence or two about:\n",
    "- what you liked or disliked,\n",
    "- found interesting, confusing, challangeing, motivating\n",
    "while working on this assignment.\n",
    "\n",
    "\n",
    "*ADD YOUR THOUGHTS HERE*\\\n",
    "\\\n",
    "I liked the coding section of the assignment and applying what I learned in class. I enjoyed interpreting the results in the questions following each modelling section of the assignment. However, I felt like the observations/interpretation part was repetitive, as I felt like I had already explained my results in the questions portion after each coding section and was just re-explaining.\n",
    "\n",
    "When I was learning these concepts in the lectures and labs, I felt overwhelmed by the loads of information being taught to us all at once. However, being able to apply the concepts we learned allowed me to solidify my understanding of the course material, leaving me feeling very motivated for future material and assignments."
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "id": "db951b3a",
   "metadata": {},
   "source": [
    "## Part 5: Bonus Question (4 marks)\n",
    "\n",
    "Repeat Part 2 with Ridge and Lasso regression to see if you can improve the accuracy results. Which method and what value of alpha gave you the best R^2 score? Is this score \"good enough\"? Explain why or why not.\n",
    "\n",
    "**Remember**: Only test values of alpha from 0.001 to 100 along the logorithmic scale."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "47623d44",
   "metadata": {},
   "outputs": [],
   "source": [
    "# TO DO: ADD YOUR CODE HERE"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "id": "1b606236",
   "metadata": {},
   "source": [
    "*ANSWER HERE*"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
